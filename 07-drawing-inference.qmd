---
title: "Drawing Inference"
author: "Lucy D'Agostino McGowan"
footer:  "[Dr. Lucy D'Agostino McGowan](https://lucymcgowan.com)"
logo: "img/favicon.png"
editor: source
format: 
  revealjs: 
    theme: [custom.scss]
    slide-number: true
    chalkboard: true
knitr:
  opts_chunk: 
    code-fold: true
    echo: true
---

```{r, child = "setup.rmd"}
```

## Data

```{r}
#| code-fold: false
library(Stat2Data)

data(Sparrows)
Sparrows %>%
  select(Weight, WingLength)
```

## When is a simple linear regression model a useful descriptive summary?

> * Linearity holds
> * The residuals have "zero mean" (this is always true!)
> * The datapoints are independent

# What if we want to draw inference on another sample?

## Inference

> * So far we've only been able to describe our sample
> * for example, we've just been describing $\hat{\beta}_1$ the estimated slope of the relationship between $x$ and $y$
> * what if we want to extend these claims to the **population**?


## Sparrows {.small}

_This data contains a sample of 116 sparrows from Kent Island._ 

```{r}
ggplot(Sparrows, aes(x = WingLength, y = Weight)) + 
  geom_point() + 
  labs(title = "The relationship between weight and wing length", 
       subtitle = "Savannah Sparrows on Kent Island",
       x = "Wing length")
```

## Sparrows {.small}

::: {.question .small}
What if this were the true population, and the sample that we saw was just related _by chance_?
:::

```{r}
#| echo: false
set.seed(1)
ggplot(Sparrows, aes(x = WingLength, y = Weight)) + 
  geom_point(data = data.frame(x = rnorm(600, 27, 4),
                               y = rnorm(600, 14, 3)),
             aes(x = x, y = y), pch = 1) +
  geom_point(color = "red") +
  labs(title = "The relationship between weight and wing length", 
       subtitle = "Savannah Sparrows on Kent Island",
       x = "Wing length")
```

## Sparrows {.small}

::: {.question .small}
Ultimately, what do we want to know?
:::

```{r}
#| fig-height: 2.5
ggplot(Sparrows, aes(x = WingLength, y = Weight)) + 
  geom_point() + 
  labs(title = "The relationship between weight and wing length", 
       subtitle = "Savannah Sparrows on Kent Island",
       x = "Wing length")
```

> * Does the slope in the **population** differ from 0?
> * Equivalently, does $\beta_1$ differ from 0?


## Sparrows {.small}

::: {.question .small}
Ultimately, what do we want to know?
:::

```{r}
#| fig-height: 2.5
ggplot(Sparrows, aes(x = WingLength, y = Weight)) + 
  geom_point() + 
  labs(title = "The relationship between weight and wing length", 
       subtitle = "Savannah Sparrows on Kent Island",
       x = "Wing length")
```

> * **null hypothesis** $H_0: \beta_1 = 0$ 
> * **alternative hypothesis** $H_A: \beta_1 \ne 0$

## Sparrows {.small}

::: {.question .small}
How can we quantify how much we'd expect the slope to differ from one random sample to another?
:::

```{r}
#| fig-height: 2.5
ggplot(Sparrows, aes(x = WingLength, y = Weight)) + 
  geom_point() + 
  labs(title = "The relationship between weight and wing length", 
       subtitle = "Savannah Sparrows on Kent Island",
       x = "Wing length") +
  geom_smooth(method = "lm", formula = "y ~ x")
```

> * We need a measure of **uncertainty**
> * How about the **standard error** of $\hat{\beta}_1$?
> * The **standard error** of $\hat{\beta_1}$ ( $SE_{\hat{\beta}_1}$ ) is how much we expect the sample slope to vary from one random sample to another.


## Sparrows {.small}

::: {.question .small}
How can we quantify how much we'd expect the slope to differ from one random sample to another?
:::

```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod)
```


---

## Sparrows {.small}

We need a **test statistic** that incorporates $\hat{\beta}_1$ and the standard error $SE_{\hat\beta_1}$


```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod)
```


> * $t = \frac{\hat\beta_1}{SE_{\hat\beta_1}}$



## Sparrows {.small}

::: {.question .small}
How do we interpret this?
:::

```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod)
```

> * "the sample slope is more than 13 standard errors above a slope of zero"


## Sparrows {.small}

::: {.question .small}
How do we know what values of this statistic are worth paying attention to?
:::

```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod)
```


> * confidence intervals
> * p-values

## {{< fa laptop >}} `Application Exercise`

1. Open your `07-appex.qmd` file
2. Use the `summary` function on the linear model you fit predicting `Price` from `Mileage` on the `PorschePrice` data.
3. What is the standard error for the coefficient for Mileage? Interpret this value.

```{r}
#| echo: false
countdown::countdown(3)
```

## Sparrows {.small}

::: {.question .small}
Where do these come from?
:::

```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod)
```


 * confidence intervals
 * p-values



## Sparrows {.small}


What if we knew what the distribution of the "statistic" would be under the null hypothesis?


```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod)
```



## Sparrows {.small}

```{r}
#| code-fold: false
null_sparrow_data <- data.frame(
  WingLength = rnorm(10, 27, 4),
  Weight = rnorm(10, 14, 3)
)
lm(Weight ~ WingLength, data = null_sparrow_data) %>%
  summary()
```


## Sparrows {.small}

```{r}
#| code-fold: false
#| code-line-numbers: "2"
null_sparrow_data <- data.frame(
  WingLength = rnorm(10, 27, 4), 
  Weight = rnorm(10, 14, 3)
)
lm(Weight ~ WingLength, data = null_sparrow_data) %>%
  summary()
```

## Sparrows {.small}

```{r}
#| code-fold: false
#| code-line-numbers: "3"
null_sparrow_data <- data.frame(
  WingLength = rnorm(10, 27, 4), 
  Weight = rnorm(10, 14, 3)
)
lm(Weight ~ WingLength, data = null_sparrow_data) %>%
  summary()
```



## Sparrows {.small}


```{r}
#| code-fold: false
gen_null_stat <- function() { 
  null_sparrow_data <- data.frame(
    WingLength = rnorm(10, 27, 4),
    Weight = rnorm(10, 14, 3)
  )
  lm(Weight ~ WingLength, data = null_sparrow_data) %>%
    broom::tidy() %>%
    filter(term == "WingLength") %>%
    select("statistic")
} 
```


```{r}
#| code-fold: false
gen_null_stat()
```

---

## Sparrows {.small}


```{r}
#| code-fold: false

gen_null_stat()
gen_null_stat()
gen_null_stat()
```


## Sparrows {.small}

```{r}
#| label: gen-null
#| cache: true
#| code-fold: false
null_stats <- map_df(1:10000, ~ gen_null_stat())
```


## Sparrows {.small}

```{r}
#| code-fold: false
#| eval : false
null_stats <- map_df(1:10000, ~ gen_null_stat())
```

```{r}
ggplot(null_stats, aes(x = statistic)) +
  geom_histogram(bins = 70) + 
  labs(title = "Histogram of statistics under the null")
```


## Sparrows {.small}

::: {.question .small}
What distribution does this look like?
:::

```{r}
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  labs(title = "Histogram of statistics under the null")
```

---

## Sparrows {.small}

::: {.question .small}
What distribution does this look like?
:::

```{r}
normal_data <- null_stats %>%
  mutate(y_t = dt(statistic, df = 18),
         y_norm = dnorm(statistic, 0, 1))
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_line(data = normal_data, aes(x = statistic, y = y_norm), color = "green") + 
  labs(title = "Histogram of statistics under the null",
       subtitle = "Overlaid with a Normal distribution")
```

> * Normal?
> * What distribution is similar to the normal but with fatter tails?

---

## Sparrows {small}

::: {.question .small}
What distribution does this look like?
:::

```{r}
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_line(data = normal_data, aes(x = statistic, y = y_t), color = "green") + 
  labs(title = "Histogram of statistics under the null",
       subtitle = "Overlaid with a t-distribution")
```

> * the *t-distribution!*
> * this is a **t-distribution** with **n-2** degrees of freedom.


## Sparrows {.small}

The distribution of test statistics we would expect given the **null hypothesis is true**, $\beta_1 = 0$, is **t-distribution** with **n-2** degrees of freedom.

```{r}
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_line(data = normal_data, aes(x = statistic, y = y_t), color = "green") + 
    labs(title = "Histogram of statistics under the null",
       subtitle = "Overlaid with a t-distribution")
```


## Sparrows {.small}

```{r}
null_stats_bigger <- data.frame(
  statistic = rt(10000, df = 114)
)
ggplot(null_stats_bigger) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
    labs(title = "Histogram of statistics under the null",
         subtitle = "t-distribution with 114 degrees of freedom")
```


```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod)
```



## Sparrows {.small}

```{r}
ggplot(null_stats_bigger) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_vline(xintercept = 13.463, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null",
         subtitle = "t-distribution with 114 degrees of freedom")
```


## Sparrows {.small}

::: {.question .small}
How can we compare this line to the distribution under the null?
:::

```{r}
ggplot(null_stats_bigger) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_vline(xintercept = 13.463, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null",
         subtitle = "t-distribution with 114 degrees of freedom")
```

> * p-value



## p-value

The probability of getting a statistic as extreme or more extreme than the observed test statistic **given the null hypothesis is true**


## Sparrows {.small}

```{r}
ggplot(null_stats_bigger) +
  geom_histogram(aes(x = statistic, y = ..density..), bins = 70) + 
  geom_vline(xintercept = 13.463, lwd = 1.5) + 
  geom_vline(xintercept = -13.463, lwd = 1.5) +
    labs(title = "Histogram of statistics under the null",
         subtitle = "t-distribution with 114 degrees of freedom")
```

```{r}
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod)
```



## Return to generated data, n = 20 {.small}

```{r}
#| code-fold: false
null_stats$shade <- ifelse(null_stats$statistic > 1.5 | null_stats$statistic < -1.5, TRUE, FALSE)
```

```{r}
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, fill = shade), bins = 70) + 
  geom_vline(xintercept = 1.5, lwd = 1.5) + 
  geom_vline(xintercept = -1.5, lwd = 1.5) +
    labs(title = "Histogram of statistics under the null") + 
  theme(legend.position = "none")
```

> * Let's say we get a statistic of **1.5** in a sample


# Let's do it in R!

## Let's do it in R! {.small}

The proportion of area less than 1.5

```{r}
null_stats$shade <- ifelse(null_stats$statistic < 1.5, TRUE, FALSE)
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, fill = shade), bins = 70) + 
  geom_vline(xintercept = 1.5, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null") + 
  theme(legend.position = "none")
```


```{r}
#| code-fold: false
pt(1.5, df = 18)
```



## Let's do it in R! {.small}

The proportion of area **greater** than 1.5

```{r}
null_stats$shade <- ifelse(null_stats$statistic > 1.5, TRUE, FALSE)
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, fill = shade), bins = 70) + 
  geom_vline(xintercept = 1.5, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null") + 
  theme(legend.position = "none")
```


```{r}
#| code-fold: false
pt(1.5, df = 18, lower.tail = FALSE)
```


## Let's do it in R! {.small}

The proportion of area **greater** than 1.5 **or** **less** than -1.5.

```{r}
null_stats$shade <- ifelse(null_stats$statistic > 1.5 | null_stats$statistic < - 1.5, TRUE, FALSE)
ggplot(null_stats) +
  geom_histogram(aes(x = statistic, fill = shade), bins = 70) + 
  geom_vline(xintercept = 1.5, lwd = 1.5) + 
    geom_vline(xintercept = -1.5, lwd = 1.5) + 
    labs(title = "Histogram of statistics under the null") + 
  theme(legend.position = "none")
```

. . .

```{r}
#| code-fold: false
pt(1.5, df = 18, lower.tail = FALSE) * 2
```


## p-value

The probability of getting a statistic as extreme or more extreme than the observed test statistic **given the null hypothesis is true**


## Hypothesis test {.small}

> * **null hypothesis** $H_0: \beta_1 = 0$ 
> * **alternative hypothesis** $H_A: \beta_1 \ne 0$
> * **p-value**: 0.15
> * Often, we have an $\alpha$-level cutoff to compare this to, for example **0.05**. Since this is greater than **0.05**, we **fail to reject the null hypothesis**


## confidence intervals

If we use the same sampling method to select different samples and computed an interval estimate for each sample, we would expect the true population parameter ( $\beta_1$ ) to fall within the interval estimates 95% of the time.


## Confidence interval {.small}




$$\Huge \hat\beta_1 \pm t^∗ \times SE_{\hat\beta_1}$$





> * $t^*$ is the critical value for the $t_{n−2}$ density curve to obtain the desired confidence level
> * Often we want a **95% confidence level**.  


## Let's do it in R! {.small}


```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod) 
```


```{r}
#| code-fold: false

qt(0.025, df = nrow(Sparrows) - 2, lower.tail = FALSE)
```


## Let's do it in R! {.small}

::: {.question}
Why 0.025?
:::

```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod) 
```


```{r}
#| code-fold: false
qt(0.025, df = nrow(Sparrows) - 2, lower.tail = FALSE)
```


## Let's do it in R! {.small}

::: {.question}
Why `lower.tail = FALSE`?
:::

```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod) 
```


```{r}
#| code-fold: false
qt(0.025, df = nrow(Sparrows) - 2, lower.tail = FALSE)
```
---

## Let's do it in R! {.small}


```{r}
#| code-fold: false
mod <- lm(Weight ~ WingLength, data = Sparrows)
summary(mod) 
```


```{r}
#| echo: false
options(digits = 3)
```

```{r}
#| code-fold: false
t_star <- qt(0.025, df = nrow(Sparrows) - 2, lower.tail = FALSE)
0.467 + t_star * 0.0347
0.467 - t_star * 0.0347
```


## confidence intervals

If we use the same sampling method to select different samples and computed an interval estimate for each sample, we would expect the true population parameter ( $\beta_1$ ) to fall within the interval estimates 95% of the time.

