---
title: "Multiple linear regression"
author: "Lucy D'Agostino McGowan"
footer:  "[Dr. Lucy D'Agostino McGowan](https://lucymcgowan.com)"
logo: "img/favicon.png"
editor: source
format: 
  revealjs: 
    theme: [custom.scss]
    slide-number: true
    chalkboard: true
knitr:
  opts_chunk: 
    code-fold: true
    echo: true
---

## Simple linear regression


```{r child = "setup.Rmd"}
```


```{r packages}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(broom)
library(Stat2Data)
library(ggdag)
library(datasauRus)
```


$$y = \beta_0 + \beta_1X + \epsilon$$
<br><br>

$$\epsilon\sim N(0,\sigma_\epsilon)$$


## Multiple linear regression

<br><br>

$$y = \beta_0 + \beta_1X_1 + \beta_2X_2+\dots+\beta_kX_k+ \epsilon$$

<br><br>

$$\epsilon\sim N(0,\sigma_\epsilon)$$



## Multiple linear regression

::: question
How are these coefficients estimated?
:::

$$\hat{y} = \hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2+\dots+\hat\beta_kX_k$$

> * estimate coefficents that minimize the sum of squared residuals



## Let's do it in R!

* Data: Porsche Price
* Price, Mileage, Age



## Let's do it in R!

::: question
What is my response variable? What are my explantory variables?
:::

* Data: Porsche Price
* Price, Mileage, Age



## Let's do it in R!


```{r}
#| code-fold: false
data("PorschePrice")
lm(Price ~ Mileage + Age, data = PorschePrice)
```




## Let's do it in R!

::: question
What is different between this and the `lm()` functions we have been previously running?
:::


```{r}
#| code-fold: false
data("PorschePrice")
lm(Price ~ Mileage + Age, data = PorschePrice)
```



## Let's do it in R! {.small}

::: question
What is different between this and the `lm()` functions we have been previously running?
:::

```{r}
#| code-fold: false
model <- lm(Price ~ Mileage + Age, data = PorschePrice)
summary(model)
```




## Let's do it in R!

::: question
How would we get the predicted values for $\hat{y}$?
:::

. . .


```{r}
#| code-fold: false
model <- lm(Price ~ Mileage + Age, data = PorschePrice)

PorschePrice <- PorschePrice %>% 
  mutate(y_hat = fitted(model)) #<<

head(PorschePrice)
```


## Let's do it in R!

::: question
The sample size is $n = 30$, what would the degrees of freedom for the SSE be now?
:::


$$\Large \sqrt{\frac{SSE}{??}}$$



## Let's do it in R!

::: question
The sample size is $n = 30$, what would the degrees of freedom for the SSE be now?
:::

$$\Large \sqrt\frac{SSE}{n - k - 1}$$



## Let's do it in R!

::: question
The sample size is $n = 30$, what would the degrees of freedom for the SSE be now?
:::


$$\Large\sqrt{ \frac{SSE}{30 - 2 - 1}}$$



## Let's do it in R!


```{r}
#| code-fold: false
lm(Price ~ Mileage + Age, data = PorschePrice) %>% 
  anova()
```



# Why might we want to do this? {.center}


## 

<img src = "img/08/flowchart.jpg" height=500>

http://science.sciencemag.org/content/347/6228/1314




# Multiple linear regression for inference {.center}



## Multiple linear regression for inference

<br><br>

**Goal:** Discover the relationship between a response (outcome, $y$), and an explanatory variable ( $x$ )



## Multiple linear regression for inference

<br><br>

**Goal:** Discover the relationship between a response (outcome, $y$), and an explanatory variable ( $x$ ) **adjusting for all known confounders**



## Multiple linear regression for inference

::: question
What is a confounder?
:::

<br><br>
**Goal:** Discover the relationship between a response (outcome, $y$), and an explanatory variable ( $x$ ) **adjusting for all known confounders**




## confounder {.center}

A confounder is a variable that is associated with both the response variable ( $y$ ) and the explanatory variable ( $x$ ). If not accounted for, it can result in seeing a spurious relationship between $x$ and $y$.



## Confounder example

```{r}
#| fig-height: 3
coords <- list(
  x = c(x = 1, y = 10, z = 5.5),
  y = c(x = 1, y = 1, z = 2)
)
dag <- dagify(y ~ x, coords = coords)
ggdag(dag) + theme_dag()
```



## Confounder example

```{r}
dag <- dagify(y ~ x, y ~ z, x ~ z, coords = coords)
ggdag(dag) + theme_dag()
```



## Confounder example

```{r}
dag <- dagify(y ~ z, x ~ z, coords = coords)
ggdag(dag) + theme_dag()
```



## Confounder example 

```{r}
#| fig-width: 8

dag <- dagify(y ~ x, coords = coords) %>%
 tidy_dagitty() %>%
 dag_label(labels = c("x" = "ice cream consumption", 
                      "y" = "crime rates",
                      "z" = "summer"))
  
ggdag(dag, text = FALSE, use_labels = "label") + theme_dag()
```




## Confounder example 

```{r}
#| fig-width: 8

dag <- dagify(y ~ z, x ~ z, coords = coords) %>%
 tidy_dagitty() %>%
 dag_label(labels = c("x" = "ice cream consumption", 
                      "y" = "crime rates",
                      "z" = "summer"))
  
ggdag(dag, text = FALSE, use_labels = "label") + theme_dag()
```



## Confounding example

<img src = "img/08/meta-analysis-confounding.png" height = 400>

Armstrong, K.A. (2012). Methods in comparative effectiveness research. _Journal of clinical oncology: official journal of the American Society of Clinical Oncology_, 30 34, 4208-14.


## A quick R aside {.small}

* So far, the data we've been using has been included in an **R package**
* To access this data we just run `data("data set")`
* What if we want to read in other data, for example from a `.csv` file?

. . .

* enter: `read_csv()`

. . .

* `read_csv()` is a function from the **readr** package, which is included when you load the **tidyverse**

. . .

* it works like this:

```{r}
#| eval: false
#| code-fold: false
df <- read_csv("the-path-to-your-file.csv")
```

Where `df` can be whatever you'd like to call your new dataset


## Berkley administration data {.small}

> * Study carried out by the graduate Division of the University of California, Berkeley in the early 70â€™s to evaluate whether there was a sex bias in graduate admissions
> * The data come from six departments. For confidentiality we'll call them A-F.
> * We have information on whether the applicant was male or female and whether they were admitted or rejected.

> * First, we will evaluate whether the percentage of males admitted is indeed higher than females, overall. Next, we will calculate the same percentage for each department.

::: footer
<span>
<img src = "img/dsbox-logo.png" width = "30"> </img> Slides adapted from <a href="https://datasciencebox.org" target="_blank">datasciencebox.org</a> by Dr. Lucy D'Agostino McGowan
</span>
:::


## {{< fa laptop >}} `Application Exercise`

1. Copy the following template into RStudio Pro:

```bash
https://github.com/sta-112-f22/appex-13.git
```

2. Run the code to calculate the proportion of each gender admitted and rejected. 
3. Create a bar plot with gender on the x-axis, filled by the % admitted (or rejected)
4. Amend the code from #2 to calculate the proportion of each gender admitted and rejected **by department**
5. Create a bar plot of the percent admitted (and rejected) by gender faceted by department

```{r}
#| echo: false
countdown::countdown(5)
```


## Berkley admin data

> * What was our response variable?
> * What was our explanatory variable of interest?
> * What was our confounder?
> * What was our equation?


## Simpson's paradox

```{r}
set.seed(1)
data <- tibble(
  x = c(rnorm(25), rnorm(25, 2), rnorm(25, 4), rnorm(25, 6)),
  group = rep(1:4, each = 25),
  y = 5 + 2 * x - 10 * group + rnorm(100, 0, 5)
)
ggplot(data, aes(x, y)) +
  geom_point() + 
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE)
```



## Simpson's paradox

```{r}
set.seed(1)
data <- tibble(
  x = c(rnorm(25), rnorm(25, 2), rnorm(25, 4), rnorm(25, 6)),
  group = rep(1:4, each = 25),
  y = 5 + 2.5 * x - 10 * group + rnorm(100, 0, 5)
)
ggplot(data, aes(x, y, color = group)) +
  geom_point() + 
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE, aes(group = group)) + 
  theme(legend.position = "none") 
```



## Porsche data


```{r, echo = TRUE}
#| code-fold: false
lm(Price ~ Mileage + Age, data = PorschePrice)
```



## Porsche data

::: question
How do you calculate a t statistic for $\hat{\beta}_2$?
:::


```{r}
#| code-fold: false
lm(Price ~ Mileage + Age, data = PorschePrice)
```




## Porsche data

::: question
How do you calculate a t statistic for $\hat{\beta}_2$?
:::


```{r}
#| code-fold: false
lm(Price ~ Mileage + Age, data = PorschePrice)
```


> * $t = \frac{\hat\beta_2}{SE_{\hat\beta_2}}$
> * $t = \frac{\hat\beta_i}{SE_{\hat\beta_i}}$


## Porsche data

::: question
What is the null and alternative hypothesis?
:::


```{r}
#| code-fold: false
lm(Price ~ Mileage + Age, data = PorschePrice) 
```


> * $\Large H_0: \beta_i = 0$
> * $\Large H_A: \beta_i \neq 0$



## Porsche data

::: question
What would the degrees of freedom be for the t-distribution used to calcualte a p-value?
:::


```{r}
#| code-fold: false
lm(Price ~ Mileage + Age, data = PorschePrice)
```


> * $n - k - 1$ = 27 




# What is that definition of a p-value again? {.center}


# What about the definition of a confidence interval? {.center}



## Porche data

::: question
How would you calculate a confidence interval for $\beta_i$?
:::


```{r}
#| code-fold: false
model <- lm(Price ~ Mileage + Age, data = PorschePrice)
confint(model)
```


> * $\hat\beta_i\pm t^*SE_{\hat\beta_i}$
> * $t^*$ is the critical value from a $t$ density with degrees of freedom equal to the error df in the model $n-k-1$, where $k$ is the number of predictors


##

<img src = "img/08/flowchart.jpg" height=500>

http://science.sciencemag.org/content/347/6228/1314




# Multiple linear regression for prediction {.center}



## Multiple linear regression for prediction
<br><br>

> * **Goal:** Discover the best model for predicting a response variable (an outcome variable, $y$) using predictors, $\mathbf{X}$
>* Ultimately, we are often _comparing_ models



## `r emo::ji("hammer_and_wrench")` toolkit for comparing models

. . .

### `r emo::ji("point_right")`  F-test

. . .

### `r emo::ji("point_right")` $\Large R^2$


## `r emo::ji("hammer_and_wrench")` F-test for Multiple Linear Regression

> * Comparing the full model to the intercept only model
> * $H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0$
> * $H_A: \textrm{at least one } \beta_i \neq 0$
> * _We will soon learn a more general version of comparing nested models_



## `r emo::ji("hammer_and_wrench")` F-test for Multiple Linear Regression

> * $F = \frac{MSModel}{MSE}$
> * df for the Model?
>     * k
> * df for the errors?
>     * n - k - 1


## `r emo::ji("hammer_and_wrench")` $R^2$ for Multiple Linear Regression

> * $R^2= \frac{SSModel}{SSTotal}$
> * $R^2= 1 - \frac{SSE}{SSTotal}$
> * As is, if you add a predictor this will _always_ increase. Therefore, we have $R^2_{adj}$ that has a small "penalty" for adding more predictors

## `r emo::ji("hammer_and_wrench")` Adjusted $R^2$ 

> * $R^2_{adj} = 1 - \frac{SSE/(n-k-1)}{SSTotal / (n-1)}$
> * $\frac{SSTotal}{n-1} = \frac{\sum(y - \bar{y})^2}{n-1}$ What is this?
>   * Sample variance! $S_Y^2$
> * $R^2_{adj} = 1 - \frac{\hat\sigma^2_\epsilon}{S_Y^2}$

## `r emo::ji("hammer_and_wrench")` $R^2_{adj}$ {.small}

* $R^2_{adj} = 1 - \frac{SSE/(n-k-1)}{SSTotal / (n-1)}$
* The denominator stays the same for all models fit to the same response variable and data
* the numerator actually increase when a new predictor is added to a model if the decrease in the SSE is not sufficient to offset the decrease in the error degrees of freedom. 
* So $R^2_{adj}$ can `r emo::ji("point_down")` when a weak predictor is added to a model


## {{< fa laptop >}} `Application Exercise`

1. Open `appex-13.qmd`
2. Using the `NFL2007Standings` data create a model that predicts `WinPct` from `PointsFor.` Examine the $R^2$ and $R^2_{adj}$ values.

3. Using the NFL2007Standings data create a model that predicts `WinPct` from `PointsFor` AND `PointsAgainst.` Examine the $R^2$ and $R^2_{adj}$ values.

4. Which model do you think is better for predicting win percent?

```{r}
#| echo: false
countdown::countdown(5)
```