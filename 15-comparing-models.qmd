---
title: "Model Comparisons"
author: "Lucy D'Agostino McGowan"
footer:  "[Dr. Lucy D'Agostino McGowan](https://lucymcgowan.com)"
logo: "img/favicon.png"
editor: source
format: 
  revealjs: 
    theme: [custom.scss]
    slide-number: true
    chalkboard: true
knitr:
  opts_chunk: 
    code-fold: true
    echo: true
---

```{r child = "setup.Rmd"}
```


```{r}
#| include: false
library(Stat2Data)
options(digits = 3)
```


## `r emo::ji("hammer_and_wrench")` toolkit for comparing models

. . .

### `r emo::ji("point_right")`  F-test

. . .

### `r emo::ji("point_right")` $\Large R^2$


## `r emo::ji("hammer_and_wrench")` F-test for Multiple Linear Regression

> * Comparing the full model to the intercept only model
> $H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0$
> $H_A: \textrm{at least one } \beta_i \neq 0$


## `r emo::ji("hammer_and_wrench")` F-test for Multiple Linear Regression

> * $\Large F = \frac{MSModel}{MSE}$
> * df for the Model?
>   * k
> * df for the errors?
>   * n - k - 1



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression {.small}

> * What does "nested" mean?
>   * You have a "small" model and a "large" model where the "small" model is completely contained in the "large" model
> * The F-test we have learned so far is one example of this, comparing:
>    * $y = \beta_0 + \epsilon$ (**small**)
>    * $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots +\beta_kx_k + \epsilon$ (**large**)
> * The full (**large**) model has $k$ predictors, the reduced (**small**) model has $k - p$ predictors



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression {.small}

> * The full (**large**) model has $k$ predictors, the reduced (**small**) model has $k - p$ predictors
> * What is $H_0$?
>   * $H_0:$ $\beta_i=0$ for all $p$ predictors being dropped from the full model
>* What is $H_A$?
>   * $H_A:$ $\beta_i\neq 0$ for at least one of the $p$ predictors dropped from the full model
> * Does the full model do a (statistically significant) better job of explaining the variability in the response than the reduced model?



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression

* The full (**large**) model has $k$ predictors, the reduced (**small**) model has $k - p$ predictors
* $F = \frac{SSMODEL_{Full} - SSMODEL_{Reduced}/p}{SSE_{Full}/n-k-1}$



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression

* Which of these are nested models?

(1) $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$  
(2) $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 * x_2 + \epsilon$  
(3) $y = \beta_0 + \beta_1 x_3 + \epsilon$  
(4) $y = \beta_0 + \beta_2 x_2 + \epsilon$  
(5) $y = \beta_0 + \beta_1 x_4 + \epsilon$  
  
. . .

((4) in (1) in (2))





## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression

(1) $y = \beta_0 + \beta_2 x_2 + \epsilon$  
(2) $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 * x_2 + \epsilon$  

> * Comparing these two models, what is $p$?
>   * $p = 2$
> * What is $k$?
>   * $k = 3$



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression

* Goal: Trying to predict the weight of fish based on their length and width


::: small
```{r}
#| code-fold: false
data("Perch")
model1 <- lm(
  Weight ~ Length + Width + Length * Width,
  data = Perch
  )
model2 <- lm(
  Weight ~ Length + Width + I(Length ^ 2) + I(Width ^ 2) + Length * Width,
  data = Perch
  )
```
:::



> * What is the equation for `model1`?
> * What is the equation for `model2`?



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression {.small}


```{r}
#| code-fold: false
data("Perch")
model1 <- lm(
  Weight ~ Length + Width + Length * Width,
  data = Perch
  )
model2 <- lm(
  Weight ~ Length + Width + I(Length ^ 2) + I(Width ^ 2) + Length * Width,
  data = Perch
  )
```


> * If we want to do a _nested F-test_, what is $H_0$?
>   * $H_0: \beta_3 = \beta_4 = 0$
> * What is $H_A$?
>   * $H_A: \beta_3\neq 0$ or $\beta_4\neq 0$
> * What are the degrees of freedom of this test? (n = 56)
>   * 2, 50



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression



```{r}
#| code-fold: false
anova(model1)
```

```{r}
#| code-fold: false
(SSModel1 <- 6118739 + 110593 + 314997)
```



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression



```{r}
#| code-fold: false
anova(model2)
```

```{r}
#| code-fold: false
(SSModel1 <- 6118739 + 110593 + 314997)
(SSModel2 <- 6118739 + 110593 + 314899 + 5381 + 3482)
```




## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression

> * $F = \frac{SSMODEL_{Full} - SSMODEL_{Reduced}/p}{SSE_{Full}/n-k-1}$
> * $SSMODEL_{Full} - SSMODEL_{Reduced}$:

. . .

```{r}
#| code-fold: false
SSModel2 - SSModel1
```

> * What is $p$?





## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression

> * $F = \frac{SSMODEL_{Full} - SSMODEL_{Reduced}/p}{SSE_{Full}/n-k-1}$
> * $SSMODEL_{Full} - SSMODEL_{Reduced}$ / p:

. . .

```{r}
#| code-fold: false
(SSModel2 - SSModel1) / 2
```



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression

* $F = \frac{SSMODEL_{Full} - SSMODEL_{Reduced}/p}{SSE_{Full}/n-k-1}$
* $SSE_{Full}/n-k-1$


```{r}
#| code-fold: false
anova(model2)
```




## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression

* $F = \frac{SSMODEL_{Full} - SSMODEL_{Reduced}/p}{SSE_{Full}/n-k-1}$

```{r}
#| code-fold: false
((SSModel2 - SSModel1) / 2) /
  1860
```


> * What are the degrees of freedom for this test?
>    * 2, 50

. . .

```{r}
#| code-fold: false
pf(2.356183, 2, 50, lower.tail = FALSE)
```



## `r emo::ji("hammer_and_wrench")` Nested F-test for Multiple Linear Regression

_An easier way_


```{r}
#| code-fold: false
anova(model1, model2)
```




## `r emo::ji("hammer_and_wrench")` $R^2$ for Multiple Linear Regression {.small}

> * $R^2= \frac{SSModel}{SSTotal}$
> * $R^2= 1 - \frac{SSE}{SSTotal}$
> * As is, if you add a predictor this will _always_ increase. Therefore, we have $R^2_{adj}$ that has a small "penalty" for adding more predictors
> * $R^2_{adj} = 1 - \frac{SSE/(n-k-1)}{SSTotal / (n-1)}$
> * $\frac{SSTotal}{n-1} = \frac{\sum(y - \bar{y})^2}{n-1}$ What is this?
>    * Sample variance! $S_Y^2$
> * $R^2_{adj} = 1 - \frac{\hat\sigma^2_\epsilon}{S_Y^2}$



## `r emo::ji("hammer_and_wrench")` $R^2_{adj}$ for Multiple Linear Regression {.small}

* $R^2_{adj} = 1 - \frac{SSE/(n-k-1)}{SSTotal / (n-1)}$
* The denominator stays the same for all models fit to the same response variable and data
* the numerator actually increase when a new predictor is added to a model if the decrease in the SSE is not sufficient to offset the decrease in the error degrees of freedom. 
* So $R^2_{adj}$ can `r emo::ji("point_down")` when a weak predictor is added to a model



## `r emo::ji("hammer_and_wrench")` $R^2_{adj}$ for Multiple Linear Regression


::: {layout-ncol=2}

::: smaller
```{r}
#| code-fold: false
summary(model1)
```
:::

::: smaller
```{r}
#| code-fold: false
summary(model2)
```
:::

:::



## Model Comparision criteria

> * We are looking for reasonable ways to balance "goodness of fit" (how well the model fits the data) with "parsimony" 
> * $R^2_{adj}$ gets at this by adding a **penalty** for adding variables
> * AIC and BIC are two more methods that balance goodness of fit and parsimony



## Log Likelihood {.small}

> * Both AIC and BIC are calculated using the **log likelihood**
> * $\log(\mathcal{L}) = -\frac{n}{2}[\log(2\pi) + \log(SSE/n) + 1]$
>    * $\log = \log_e$, `log()` in `R`

. . .

::: small
```{r}
#| code-fold: false
logLik(model1)
-56 / 2 * (log(2 * pi) + log(101765 / 56) + 1)
```
:::


> * "goodness of fit" measure
> * **higher** log likelihood is better


## Log Likelihood {.small}

**What I want you to remember**

* Both AIC and BIC are calculated using the **log likelihood**

$$\log(\mathcal{L}) = -\frac{n}{2}[\log(SSE/n) ]+\textrm{some constant}$$

> * $\log = \log_e$, `log()` in `R`
> * "goodness of fit" measure
> * **higher** log likelihood is better


## AIC {.small}

* Akaike's Information Criterion
* $AIC = 2(k+1) - 2\log(\mathcal{L})$
* $k$ is the number of predictors in the model
* **lower** AIC values are better

```{r}
#| code-fold: false
AIC(model1)
AIC(model2)
```



## BIC {.small}

* Bayesian Information Criterion
* $BIC = \log(n)(k+1) - 2\log(\mathcal{L})$
* $k$ is the number of predictors in the model
* **lower** BIC values are better


```{r}
#| code-fold: false
BIC(model1)
BIC(model2)
```




## AIC and BIC can disagree! {.small}


```{r}
#| code-fold: false
AIC(model1)
AIC(model2)
BIC(model1)
BIC(model2)
```

> * the penalty term is larger in BIC than in AIC
> * What to do? Both are valid, **pre-specify** which you are going to use before running your models in the **methods** section of your analysis


## `r emo::ji("hammer_and_wrench")` toolkit for comparing models

### `r emo::ji("point_right")`  F-test
### `r emo::ji("point_right")` $\Large R^2$
### `r emo::ji("point_right")` $AIC$
### `r emo::ji("point_right")` $BIC$


## {{< fa laptop >}} `Application Exercise`

::: smaller

1. Copy the following template into RStudio Pro:

```bash
https://github.com/sta-112-f22/appex-15.git
```

2.  Fit a model predicting `GPA` using high school gpa (`HSGPA`) and verbal SAT score (`SATV`). Save this as `model1`

3. Fit a model predicting `GPA` using high school gpa, verbal SAT score (`SATV`), and math SAT score (`SATM`). Save this as `model2`.

4. Fit a model predicting `GPA` using high school gpa, verbal SAT score (`SATV`), math SAT score (`SATM`), and number of humanities credits taken in high school (`HU`). Save this as `model3`.

5. Conduct an "nested F test" comparing Model 1 to Model 2. What are the null and alternative hypotheses? Is model 2 significantly better than model 1?

6. Conduct a "nested F test" comparing Model 2 to Model 3. What are the null and alternative hypotheses? Is model 3 significantly better than model 2?

7. Choose AIC or BIC to compare models 1, 2, and 3. Rank the models.

:::

```{r}
#| echo: false
countdown::countdown(8)
```



