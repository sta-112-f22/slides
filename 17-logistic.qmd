---
title: "Interpreting coefficients"
author: "Lucy D'Agostino McGowan"
footer:  "[Dr. Lucy D'Agostino McGowan](https://lucymcgowan.com)"
logo: "img/favicon.png"
editor: source
format: 
  revealjs: 
    theme: [custom.scss]
    slide-number: true
    chalkboard: true
knitr:
  opts_chunk: 
    code-fold: true
    echo: true
---

## Outcome variable


```{r child = "setup.Rmd"}
```

```{r}
#| include: false
library(Stat2Data)
```

> * So far, we've only had **continuous** (numeric, quantitative) outcome variables ( $y$ )
> * We've just learned about **categorical** and **binary** explanatory variables ( $x$ )
> * What if we have a **binary** outcome variable?



## Outcome variable

::: question
What does it mean to be a **binary** variable?
:::

* So far, we've only had **continuous** (numeric, quantitative) outcome variables ( $y$ )
* We've just learned about **categorical** and **binary** explanatory variables ( $x$ )
* What if we have a **binary** outcome variable?



## Let's look at an example {.small}

* 446 teens were asked "On an average school night, do you get at least 7 hours of sleep"
* Outcome is [1 = "Yes", 0 = "No"]
* Is Age related to this outcome?

. . .

* What if I try to fit this as a _linear regression_ model?

. . .

```{r, fig.height = 2}
data("LosingSleep")

ggplot(LosingSleep, aes(Age, Outcome)) + 
  geom_point() + 
  geom_line(aes(x = Age, y = predict(lm(Outcome ~ Age, data = LosingSleep))))
```


## Let's look at an example {.small}

* 446 teens were asked "On an average school night, do you get at least 7 hours of sleep"
* Outcome is [1 = "Yes", 0 = "No"]
* Is Age related to this outcome?
* What if I try to fit this as a _linear regression_ model?

```{r, fig.height = 2}
LosingSleep %>%
  group_by(Age) %>%
  count(Age, Outcome) %>%
  mutate(p = n / sum(n)) %>%
  filter(Outcome == 1) %>%
  ggplot(aes(Age, p)) + 
  geom_point() + 
  geom_line(data = LosingSleep, aes(x = Age, y = predict(lm(Outcome ~ Age, data = LosingSleep)))) + 
  ylim(0, 1)
```


## Let's look at an example {.small}

* 446 teens were asked "On an average school night, do you get at least 7 hours of sleep"
* Outcome is [1 = "Yes", 0 = "No"]
* Is Age related to this outcome?
* What if I try to fit this as a _linear regression_ model?

```{r, fig.height = 2}
new_data <- data.frame(Age = 0:40)
LosingSleep %>%
  group_by(Age) %>%
  count(Age, Outcome) %>%
  mutate(p = n / sum(n)) %>%
  filter(Outcome == 1) %>%
  ggplot(aes(Age, p)) + 
  geom_point() + 
  geom_line(data = new_data, aes(x = Age, y = predict(lm(Outcome ~ Age, data = LosingSleep), newdata = new_data))) + 
  ylim(-1, 2) + 
  xlim(0, 40) +
  geom_hline(yintercept = c(0, 1), lty = 2) 
```



## Let's look at an example

* Perhaps it would be sensible to find a function that would not extend beyond 0 and 1?

```{r, fig.height = 2}
LosingSleep %>%
  group_by(Age) %>%
  count(Age, Outcome) %>%
  mutate(p = n / sum(n)) %>%
  filter(Outcome == 1) %>%
  ggplot(aes(Age, p)) + 
  geom_point() + 
  geom_rect(aes(xmin = 0, xmax = 5, ymin = 1, ymax = 1.2), fill = "yellow", color = "black") +
  geom_rect(aes(xmin = 35, xmax = 40, ymin = 0, ymax = -0.2), fill = "yellow", color = "black") +
  geom_line(data = new_data, aes(x = Age, y = predict(lm(Outcome ~ Age, data = LosingSleep), newdata = new_data))) + 
  ylim(-1, 2) + 
  xlim(0, 40) +
  geom_hline(yintercept = c(0, 1), lty = 2) 
```



## Let's look at an example

* Perhaps it would be sensible to find a function that would not extend beyond 0 and 1?

```{r, fig.height = 2}
LosingSleep %>%
  group_by(Age) %>%
  count(Age, Outcome) %>%
  mutate(p = n / sum(n)) %>%
  filter(Outcome == 1) %>%
  ggplot(aes(Age, p)) + 
  geom_point() + 
  geom_rect(aes(xmin = 0, xmax = 5, ymin = 1, ymax = 1.2), fill = "yellow", color = "black") +
  geom_rect(aes(xmin = 35, xmax = 40, ymin = 0, ymax = -0.2), fill = "yellow", color = "black") +
  geom_line(data = new_data,
            aes(x = Age, y = predict(
              glm(Outcome ~ Age, family = "binomial", data = LosingSleep),
              newdata = new_data,
              type = "response"))) + 
  ylim(-1, 2) + 
  xlim(0, 40) +
  geom_hline(yintercept = c(0, 1), lty = 2) 
```



## Let's look at an example

* Perhaps it would be sensible to find a function that would not extend beyond 0 and 1?

```{r, fig.height = 2}
LosingSleep %>%
  group_by(Age) %>%
  count(Age, Outcome) %>%
  mutate(p = n / sum(n)) %>%
  filter(Outcome == 1) %>%
  ggplot(aes(Age, p)) + 
  geom_point() + 
  geom_line(data = new_data,
            aes(x = Age, y = predict(
              glm(Outcome ~ Age, family = "binomial", data = LosingSleep),
              newdata = new_data,
              type = "response"))) + 
  xlim(0, 40) +
  geom_hline(yintercept = c(0, 1), lty = 2) 
```

* this line is fit using **logistic** regression model



## How does this compare to linear regression? {.small}

Model | Outcome | Form
------|---------|-------
Ordinary linear Regression | Numeric | $y \approx \beta_0 + \beta_1 x$
Number of Doctors example | Numeric | $\sqrt{\textrm{Number of doctors}}\approx \beta_0 +\beta_1x$
Logistic regression | Binary| $\log\left(\frac{\pi}{1-\pi}\right) \approx \beta_0 + \beta_1x$


> * $\pi$ is the probability that $y = 1$ ( $P(y = 1)$ )



## Notation {.small}

> * $\log\left(\frac{\pi}{1-\pi}\right)$: the "log odds"
> * $\pi$ is the **probability** that $y = 1$ - the probability that your outcome is 1.
> * $\frac{\pi}{1-\pi}$ is a ratio representing the **odds** that $y = 1$ 
> * $\log\left(\frac{\pi}{1-\pi}\right)$ is the **log odds**
> * The **transformation** from $\pi$ to $\log\left(\frac{\pi}{1-\pi}\right)$ is called the logistic or **logit** transformation


## A bit about "odds" {.small}

> * The "odds" tell you how likely an event is
> * `r emo::ji("coin")` if I flip a fair coin, what is the probability that I'd get **heads**?
>   * $p = 0.5$
> * What is the probability that I'd get **tails**?
>   * $1 - p = 0.5$
> * The **odds** are 1:1, 0.5:0.5
> * the **odds** can be written as $\frac{p}{1-p} =\frac{0.5}{0.5} = 1$



## A bit about "odds" {.small}

* The "odds" tell you how likely an event is

> * `r emo::ji("rain")` Let's say there is a 60% chance of rain today  
> * What is the probability that it will rain?
>    * $p = 0.6$
> * What is the probability that it **won't** rain?
>    * $1-p = 0.4$
> * What are the **odds** that it will rain? 
>    * 3 to 2, 3:2, $\frac{0.6}{0.4} = 1.5$
  

## Transforming logs {.small}

> * How do you "undo" a $\log$ base $e$?
> * Use $e$! For example:
>    * $e^{\log(10)} = 10$
> * $e^{\log(1283)} = 1283$
> * $e^{\log(x)} = x$
  


## Transforming logs {.small}

::: question
How would you get the **odds** from the **log(odds)**?
::: 

* How do you "undo" a $\log$ base $e$?
* Use $e$! For example:
  * $e^{\log(10)} = 10$ 
  * $e^{\log(1283)} = 1283$
  * $e^{\log(x)} = x$
  
. . .

* $e^{\log(odds)}$ = odds



## Transforming odds {.small}

* odds = $\frac{\pi}{1-\pi}$
* Solving for $\pi$
  * $\pi = \frac{\textrm{odds}}{1+\textrm{odds}}$

. . .

* Plugging in $e^{\log(odds)}$ = odds

. . .

  * $\pi = \frac{e^{\log(odds)}}{1+e^{\log(odds)}}$

. . .

* Plugging in $\log(odds) = \beta_0 + \beta_1x$

. . .

  * $\pi = \frac{e^{\beta_0 + \beta_1x}}{1+e^{\beta_0 + \beta_1x}}$



## The logistic model {.small}

* `r emo::ji("v")` forms

Form | Model
----|-----
Logit form | $\log\left(\frac{\pi}{1-\pi}\right) = \beta_0 + \beta_1x$
Probability form | $\Large\pi = \frac{e^{\beta_0 + \beta_1x}}{1+e^{\beta_0 + \beta_1x}}$



## The logistic model {.small}

probability | odds | log(odds)
-----|-------|-----
$\pi$ | $\frac{\pi}{1-\pi}$ | $\log\left(\frac{\pi}{1-\pi}\right)=l$

. . . 


### `r emo::ji("left_arrow")`


log(odds) | odds | probability
-----------|-----|---------
$l$ | $e^l$ | $\frac{e^l}{1+e^l} = \pi$



## The logistic model {.small}

> * `r emo::ji("v")` forms
> * **log(odds)**: $l \approx \beta_0 + \beta_1x$
> * **P(Outcome = Yes)**: $\Large\pi \approx\frac{e^{\beta_0 + \beta_1x}}{1+e^{\beta_0 + \beta_1x}}$



## Example

* We are interested in the probability of getting accepted to medical school given a college student's GPA

::: question
```{r, fig.height = 2}
#| code-fold: false
data("MedGPA")
ggplot(MedGPA, aes(Accept, GPA)) + 
  geom_boxplot() + 
  geom_jitter()
```
:::




## Example {.small}

::: question
What is the equation for the model we are going to fit?
:::

* We are interested in the probability of getting accepted to medical school given a college student's GPA



## Example {.small}

::: question
What is the equation for the model we are going to fit?
:::

* $\log(odds) = \beta_0 + \beta_1 GPA$
* P(Accept) $\approx \frac{e^{\beta_0 + \beta_1GPA}}{1+e^{\beta_0 + \beta_1GPA}}$
* We are interested in the probability of getting accepted to medical school given a college student's GPA



## Example {.small}

* We are interested in the probability of getting accepted to medical school given a college student's GPA

::: small

```{r}
#| code-fold: false
glm(Accept ~ GPA, data = MedGPA,
    family = "binomial") 
```
:::



## Example {.small}

* We are interested in the probability of getting accepted to medical school given a college student's GPA


```{r}
#| code-fold: false
glm(Accept ~ GPA, data = MedGPA,
    family = "binomial") %>%
  fitted() 
```



